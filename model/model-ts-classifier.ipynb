{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model built with raw TensorFlow\n",
    "\n",
    "This model is an attempt to drop HuggingFace transformers library and use pure TensorFlow code.\n",
    "\n",
    "The goal is to get a single source of truth and to use directly Google's models.\n",
    "\n",
    "Adapted from: <https://www.tensorflow.org/text/tutorials/classify_text_with_bert>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflowjs as tfjs\n",
    "#from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "# Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Custom\n",
    "from helper_functions import load_sst_dataset, plot_model_history, save_ts_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TensorFlow to log only the errors\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Force the use of the CPU instead of the GPU if running out of GPU memory\n",
    "device = '/CPU:0' # input '/CPU:0' to use the CPU or '/GPU:0' for the GPU\n",
    "\n",
    "# Model to be used\n",
    "bert_model_name         = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n",
    "tfhub_handle_encoder    = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "\n",
    "# Tokenizing parameters\n",
    "max_length = 60    # Max length of an input\n",
    "\n",
    "# Training parameters\n",
    "epochs = 1  # 1 is enough for code testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset sst (C:\\Users\\thiba\\.cache\\huggingface\\datasets\\sst\\default\\1.0.0\\b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 54.55it/s]\n"
     ]
    }
   ],
   "source": [
    "text_train, Y1, Y2, text_test, Y1_test, Y2_test = load_sst_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "\n",
    "  # Input\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "  # Preprocessing \n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "\n",
    "  # Encoder\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "\n",
    "  # Encoder's output\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "\n",
    "  # Regression\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "\n",
    "  # Return the model\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps_per_epoch = len(Y2)\n",
    "#num_train_steps = steps_per_epoch * epochs\n",
    "#num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "#init_lr = 3e-5\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=5e-05,\n",
    "        epsilon=1e-08,\n",
    "        decay=0.01,\n",
    "        clipnorm=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input\n",
    "x = {'text': tf.convert_to_tensor(text_train)}\n",
    "\n",
    "# Training output\n",
    "y = {'classifier': Y2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 126s 1s/step - loss: 0.7322 - binary_accuracy: 0.4745 - val_loss: 0.8285 - val_binary_accuracy: 0.6630\n"
     ]
    }
   ],
   "source": [
    "# doc: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "history = classifier_model.fit(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    validation_split=0.2,\n",
    "    batch_size=64,\n",
    "    epochs=epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 26s 246ms/step - loss: 0.6948 - binary_accuracy: 0.6059\n"
     ]
    }
   ],
   "source": [
    "# Test input\n",
    "x_test = {'text': tf.convert_to_tensor(text_test)}\n",
    "\n",
    "# Test output\n",
    "y_test = {'classifier': Y2_test}\n",
    "\n",
    "model_eval = classifier_model.evaluate(\n",
    "    x=x_test,\n",
    "    y=y_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 165). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "classifier_model.save(\"./test_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjs.converters.save_keras_model(classifier_model, \"./test_tfjs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see: <https://www.tensorflow.org/js/tutorials/conversion/import_keras>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bda197ddedf6ec285a187733776d3e3dbe80c997e28ce01a7c84215752101f1a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('bert': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
